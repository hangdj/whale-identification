{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training image directory path\n",
    "TRAIN_DIR = './data/train/'\n",
    "# preprocessed training image directory path\n",
    "TRAIN_CLEAN_DIR = './data/train_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "labels = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Model Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large fraction of the images are either grayscale or binary (black and white).\n",
    "The simplest option here is to convert the whole image corpus to grayscale in order to avoid various issues that may arise due to representation non-conformity. However, chroma information can possibly be a useful discrimination aid for the majority of images which do contain it. Hence, we'll avoid discarding it by augmenting our dataset with grayscale images to reduce learned reliance on color features. We'll handle the last issue in the augmentation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# cv2.imread(path,cv2.IMREAD_COLOR) implicitly upchannels grayscale images to BGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll resize to 700x1050 pixels (most samples are near these dimensions) by zero padding to a 3/2 aspect ratio, then do the actual resizing via `cv2.resize` (linear interpolation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_RATIO = 3/2\n",
    "def get_padded_dimensions(img):\n",
    "    h,w,d = img.shape\n",
    "    if w/h > ASPECT_RATIO:\n",
    "        h = int(np.round(w/ASPECT_RATIO))\n",
    "    else: \n",
    "        w = int(h*ASPECT_RATIO)\n",
    "    return h,w,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "700x1050 pixels is too many features to compute from in a short amount of time (given our limited resources). We'll use 140x210 pixel images (roughly in-line with ImageNet dataset dimensions) to enable faster training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll mostly ignore the `new_whale` class since it's both too coarsly defined and composes the majority of the data - this might lead to model confusion. Furthermore, there's enough data to provide ample negative and positive examplars whenafter discarding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[labels.Id != 'new_whale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_file_path):\n",
    "    # implicitly upchannels grayscale images\n",
    "    img = cv2.imread(img_file_path, cv2.IMREAD_COLOR)\n",
    "    # get padded image dimensions\n",
    "    h,w,d = get_padded_dimensions(img)\n",
    "    # apply padding\n",
    "    new_img = np.zeros((h,w,d),dtype=np.uint8)\n",
    "    new_img[:img.shape[0],:img.shape[1],:] = img\n",
    "    # downsample\n",
    "    new_img = cv2.resize(new_img, (210,140))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to training set\n",
    "for img_name in labels.Image:\n",
    "    img =  preprocess(TRAIN_DIR+img_name)\n",
    "    cv2.imwrite(TRAIN_CLEAN_DIR+img_name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant number of known whales show up in only a single image. We'll augment the dataset by duplicating and distorting images of single image sample labels. We mostly want to generate enough samples to reduce chromatic bias/confusion and generate a second sample for the single image sample classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on code from https://github.com/mdbloice/Augmentor\n",
    "def perspective_augmentation(img): \n",
    "    h,w = img.shape[:2]\n",
    "    # randomly select the skew amount\n",
    "    skew_amount = np.random.randint(1, max(w, h))\n",
    "    # image corner positions\n",
    "    x1 = 0; x2 = h; y1 = 0; y2 = w\n",
    "    original_plane = [(y1, x1), (y2, x1), (y2, x2), (y1, x2)]\n",
    "    # randomly select a direction tilt direction\n",
    "    skew_direction = np.random.randint(0, 3)\n",
    "    if skew_direction == 0:\n",
    "        # Left Tilt\n",
    "        new_plane = [(y1, x1 - skew_amount),  # Top Left\n",
    "                     (y2, x1),                # Top Right\n",
    "                     (y2, x2),                # Bottom Right\n",
    "                     (y1, x2 + skew_amount)]  # Bottom Left\n",
    "    elif skew_direction == 1:\n",
    "        # Right Tilt\n",
    "        new_plane = [(y1, x1),                # Top Left\n",
    "                     (y2, x1 - skew_amount),  # Top Right\n",
    "                     (y2, x2 + skew_amount),  # Bottom Right\n",
    "                     (y1, x2)]                # Bottom Left\n",
    "    elif skew_direction == 2:\n",
    "        # Forward Tilt\n",
    "        new_plane = [(y1 - skew_amount, x1),  # Top Left\n",
    "                     (y2 + skew_amount, x1),  # Top Right\n",
    "                     (y2, x2),                # Bottom Right\n",
    "                     (y1, x2)]                # Bottom Left\n",
    "    elif skew_direction == 3:\n",
    "        # Backward Tilt\n",
    "        new_plane = [(y1, x1),                # Top Left\n",
    "                     (y2, x1),                # Top Right\n",
    "                     (y2 + skew_amount, x2),  # Bottom Right\n",
    "                     (y1 - skew_amount, x2)]  # Bottom Left\n",
    "    # calculate perspective transform matrix coefficients\n",
    "    M = cv2.getPerspectiveTransform(np.float32(original_plane), np.float32(new_plane))\n",
    "    # also do a zoom augmentation (i.e scale transform) while we're at it \n",
    "    zoom_coefficient = np.random.uniform(0.8,1.2)\n",
    "    M[0,0] = M[0,0]*zoom_coefficient\n",
    "    M[1,1] = M[1,1]*zoom_coefficient\n",
    "    # apply perspective transform\n",
    "    dst = cv2.warpPerspective(img,M,(210,140))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image    5004\n",
       "Id       5004\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a single sample from each class\n",
    "unique_labels = labels.drop_duplicates(subset=['Id'])\n",
    "unique_labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl in unique_labels.iterrows():\n",
    "    img = cv2.imread(TRAIN_CLEAN_DIR+lbl.Image, cv2.IMREAD_GRAYSCALE)  # load as monochrome \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # upchannel to BGR\n",
    "    # perspective and zoom augmentation\n",
    "    img = perspective_augmentation(img)\n",
    "    # create augmented image entry\n",
    "    cv2.imwrite(TRAIN_CLEAN_DIR+lbl.Image, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add augmented images to label data\n",
    "unique_labels.Image = 'aug_'+unique_labels.Image\n",
    "labels.append(unique_labels)\n",
    "labels = labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.to_csv('./data/clean_labels.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv-course-env]",
   "language": "python",
   "name": "conda-env-cv-course-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
